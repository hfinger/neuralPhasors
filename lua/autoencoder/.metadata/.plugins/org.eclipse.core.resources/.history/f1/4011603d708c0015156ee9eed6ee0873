require 'unsup';
require 'nn';
require 'gnuplot';
require 'Encoder';
require 'Decoder';
require 'image';
require 'optim';
require 'functions'
require 'randomkit'
require 'lapp'

--load Data
traindata, testdata = loadData{normalizeMean = false} 

opt = lapp[[
   -f,--full                                use the full dataset
   -p,--plot                                plot while training
   -o,--optimization  (default "SGD")       optimization: SGD | LBFGS 
   -r,--learningRate  (default 0.05)        learning rate, for SGD only
   -b,--batchSize     (default 10)          batch size
   -m,--momentum      (default 0)           momentum, for SGD only
   -h,--hidden        (default 50)          number of hidden units
   -k,--kernel        (default 7)           kernelsize
   --coefL1           (default 0)           L1 penalty on the weights
   --coefL2           (default 0)           L2 penalty on the weights
   -t,--threads       (default 4)           number of threads
]]

n_hidden = arg[1] or 100
kernelsize = arg[2] or 7
n_input = 32*32
trainingSteps = arg[3] or 5000
batchsize = arg[4] or 64
config = {learningRate = arg[5] or 0.1,
         weightDecay = arg[6] or 5e-5,
         momentum = 0,
         learningRateDecay = arg[7] or 5e-7}
         
--initialize Encode
enc = nn.Encoder(1,n_hidden,kernelsize)
dec = nn.Decoder(n_hidden,1,kernelsize)


autoencoder = nn.Sequential()
autoencoder:add(enc)
autoencoder:add(dec)

--set Criterion 
criterion = nn.ParallelCriterion()

xcrit = nn.MSECriterion() --compare x_in and x_out
ycrit = nn.MSECriterion() --compare y_in and y_out

criterion:add(xcrit,0.5)
criterion:add(ycrit,0.5)

parameters, gradParameters = autoencoder:getParameters()

timer = torch.Timer()
print("start training")
trainModel(trainingSteps, batchsize, traindata, autoencoder, criterion, parameters, gradParameters, config, false, 0.2)
print('finished training after ' .. timer:time().real .. ' - save model')
torch.save('model.net',autoencoder)




