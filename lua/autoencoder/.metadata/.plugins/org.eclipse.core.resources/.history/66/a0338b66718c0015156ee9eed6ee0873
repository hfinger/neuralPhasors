require 'unsup';
require 'nn';
require 'gnuplot';
require 'Encoder';
require 'Decoder';
require 'image';
require 'optim';
require 'functions'
require 'randomkit'
require 'lapp'

--load Data
traindata, testdata = loadData{normalizeMean = false} 

opt = lapp[[
   -f,--full          (default 5000)        use the full dataset or only n samples
   -o,--optimization  (default "SGD")       optimization: SGD | LBFGS 
   -r,--learningRate  (default 0.05)        learning rate, for SGD only
   -b,--batchSize     (default 10)          batch size
   -m,--momentum      (default 0)           momentum, for SGD only
   -d,--weightDecay   (default 5e-5)        weight decay for SGD
   -h,--hidden        (default 50)          number of hidden units
   -k,--kernel        (default 7)           kernelsize
   -p,--phase         (default false)       training with/without phase
   -n,--noise         (default 0.2)         noise level for denoising         
   --coefL1           (default 0)           L1 penalty on the weights
   --coefL2           (default 0)           L2 penalty on the weights
]]


n_input = 32*32
config = {learningRate = opt.learningRate,
         weightDecay = opt.weightDecay,
         momentum = opt.momentum,
         learningRateDecay = 5e-7}
         
--initialize Encode
enc = nn.Encoder(1,opt.hidden,opt.kernel)
dec = nn.Decoder(opt.hidden,1,opt.kernel)


autoencoder = nn.Sequential()
autoencoder:add(enc)
autoencoder:add(dec)

--set Criterion 
criterion = nn.ParallelCriterion()

xcrit = nn.MSECriterion() --compare x_in and x_out
ycrit = nn.MSECriterion() --compare y_in and y_out

criterion:add(xcrit,0.5)
criterion:add(ycrit,0.5)

parameters, gradParameters = autoencoder:getParameters()

timer = torch.Timer()
print("start training")
trainModel(opt.full, opt.batchsize, traindata, autoencoder, criterion, parameters, gradParameters, config, opt.phase, opt.noise)
print('finished training after ' .. timer:time().real .. ' - save model')
torch.save('model.net',autoencoder)




